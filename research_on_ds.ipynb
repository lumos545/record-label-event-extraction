{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "JovkdxvtoYLp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JovkdxvtoYLp",
    "outputId": "14f025b1-79a1-40a8-c42e-4372b90e5d86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ea075c8-fcd4-425c-9d27-558e9c3a43c8",
   "metadata": {
    "id": "2ea075c8-fcd4-425c-9d27-558e9c3a43c8"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4138a0-9922-462a-a565-c689d06e0728",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe4138a0-9922-462a-a565-c689d06e0728",
    "outputId": "c5303714-7ba7-4a08-e886-e6c02cb78c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample cleaned data ===\n",
      "                    name                                            profile  \\\n",
      "0               Planet E  Carl Craig's classic techno label founded in 1...   \n",
      "1  Earthtones Recordings  California deep house label founded by Jamie T...   \n",
      "2     Seasons Recordings  California deep-house label founded by Jamie T...   \n",
      "\n",
      "                                             contact  \n",
      "0  Planet E Communications P.O. Box 27218 Detroit...  \n",
      "1  Seasons Recordings 2236 Pacific Avenue Suite D...  \n",
      "2  Seasons Recordings Costa Mesa, CA 92627 Owner ...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/workspace/discogs_labels_cleaned.csv\")\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean label-related text while preserving names inside Discogs link formats.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Decode HTML entities\n",
    "    text = html.unescape(text)\n",
    "\n",
    "    # Remove if the value is numeric\n",
    "    text = re.sub(r'\\[a=\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\[l=\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\[r=\\d+\\]', '', text)\n",
    "\n",
    "    # Keep name if value is non-numeric\n",
    "    text = re.sub(r'\\[a=([^\\]]+)\\]', r'\\1', text)\n",
    "    text = re.sub(r'\\[l=([^\\]]+)\\]', r'\\1', text)\n",
    "    text = re.sub(r'\\[r=([^\\]]+)\\]', r'\\1', text)\n",
    "\n",
    "    # Handle [url=xxx]TEXT[/url]\n",
    "    text = re.sub(r'\\[url=[^\\]]+\\](.*?)\\[/url\\]', r'\\1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove [b] bold tags\n",
    "    text = re.sub(r'\\[/?b\\]', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove all other bracket tags\n",
    "    text = re.sub(r'\\[\\s*\\d[\\d\\s]*\\]', '', text)\n",
    "    text = re.sub(r'\\[[0-9A-Za-z \\-]{2,15}\\]', '', text)\n",
    "\n",
    "    # Handle HTML links\n",
    "    if '<' in text and '>' in text:\n",
    "        soup = BeautifulSoup(text, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to selected columns\n",
    "columns_to_clean = ['profile', 'contact', 'name']\n",
    "for col in columns_to_clean:\n",
    "    df[col] = df[col].apply(clean_text)\n",
    "\n",
    "# Preview cleaned results\n",
    "print(\"\\n=== Sample cleaned data ===\")\n",
    "print(df[['name', 'profile', 'contact']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a302d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# !python -m spacy download -q en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "YEAR   = r\"(?:19|20)\\d{2}\"\n",
    "DECADE = r\"(?:19|20)\\d0s\"\n",
    "YEAR_OR_DECADE = rf\"(?:{YEAR}s?|{DECADE})\"\n",
    "\n",
    "# Month mapping\n",
    "MONTH_MAP = {\n",
    "    \"jan\":1,\"january\":1,\"feb\":2,\"february\":2,\"mar\":3,\"march\":3,\"apr\":4,\"april\":4,\n",
    "    \"may\":5,\"jun\":6,\"june\":6,\"jul\":7,\"july\":7,\"aug\":8,\"august\":8,\"sep\":9,\"sept\":9,\n",
    "    \"september\":9,\"oct\":10,\"october\":10,\"nov\":11,\"november\":11,\"dec\":12,\"december\":12\n",
    "}\n",
    "\n",
    "_MONTH_YEAR_1 = re.compile(r\"\\b([A-Za-z]{3,9})\\s+((?:19|20)\\d{2})\\b\")\n",
    "_MONTH_YEAR_2 = re.compile(r\"\\b((?:19|20)\\d{2})\\s+([A-Za-z]{3,9})\\b\")\n",
    "_MONTH_YEAR_3 = re.compile(r\"\\b((?:19|20)\\d{2})[-/\\.](0?[1-9]|1[0-2])\\b\")\n",
    "\n",
    "_DECADE_ONLY = re.compile(rf\"\\b{DECADE}\\b\", flags=re.IGNORECASE)\n",
    "_YEAR_ONLY   = re.compile(rf\"\\b{YEAR}\\b\")\n",
    "\n",
    "def _try_parse_month_year(text: str):\n",
    "    \"\"\"\n",
    "    Try to parse month-level granularity. Returns (year, month) or (None, None).\n",
    "    \"\"\"\n",
    "    m = _MONTH_YEAR_1.search(text)\n",
    "    if m:\n",
    "        mon, year = m.group(1).lower().strip(\". \"), int(m.group(2))\n",
    "        if mon in MONTH_MAP:\n",
    "            return year, MONTH_MAP[mon]\n",
    "    m = _MONTH_YEAR_2.search(text)\n",
    "    if m:\n",
    "        year, mon = int(m.group(1)), m.group(2).lower().strip(\". \")\n",
    "        if mon in MONTH_MAP:\n",
    "            return year, MONTH_MAP[mon]\n",
    "    m = _MONTH_YEAR_3.search(text)\n",
    "    if m:\n",
    "        year, mon = int(m.group(1)), int(m.group(2))\n",
    "        return year, mon\n",
    "    return None, None\n",
    "\n",
    "def _parse_decade(s: str):\n",
    "    \"\"\"\n",
    "    Convert '1990s' -> (1990, 1999).\n",
    "    \"\"\"\n",
    "    dm = _DECADE_ONLY.search(s)\n",
    "    if not dm:\n",
    "        return None, None\n",
    "    base = int(dm.group(0)[:4])\n",
    "    return base, base + 9\n",
    "\n",
    "# Normalize heterogeneous time expressions to a unified dict\n",
    "def normalize_time(text: str):\n",
    "    # Range (including 'since X' -> (X, None))\n",
    "    s, e = extract_range_from_text(text or \"\")\n",
    "    if s:\n",
    "        # range start/end may be '1990s' or a year\n",
    "        # prefer year if possible\n",
    "        # try decade first\n",
    "        if re.fullmatch(DECADE, str(s), flags=re.IGNORECASE):\n",
    "            ds, de = _parse_decade(s)\n",
    "            sY = ds\n",
    "        else:\n",
    "            sY = int(re.search(YEAR, s).group(0)) if re.search(YEAR, s) else None\n",
    "\n",
    "        eY = None\n",
    "        if e:\n",
    "            if re.fullmatch(DECADE, str(e), flags=re.IGNORECASE):\n",
    "                ds, de = _parse_decade(e)\n",
    "                eY = de\n",
    "            else:\n",
    "                eY = int(re.search(YEAR, e).group(0)) if re.search(YEAR, e) else None\n",
    "\n",
    "        return {\n",
    "            \"start_year\": sY, \"end_year\": eY,\n",
    "            \"start_month\": None, \"end_month\": None,\n",
    "            \"granularity\": \"range\"\n",
    "        }\n",
    "\n",
    "    # Month-level (e.g., \"March 2004\", \"2004-03\")\n",
    "    y, m = _try_parse_month_year(text or \"\")\n",
    "    if y and m:\n",
    "        return {\n",
    "            \"start_year\": y, \"end_year\": y,\n",
    "            \"start_month\": m, \"end_month\": m,\n",
    "            \"granularity\": \"month\"\n",
    "        }\n",
    "\n",
    "    # Decade (e.g., \"1990s\")\n",
    "    ds, de = _parse_decade(text or \"\")\n",
    "    if ds:\n",
    "        return {\n",
    "            \"start_year\": ds, \"end_year\": de,\n",
    "            \"start_month\": None, \"end_month\": None,\n",
    "            \"granularity\": \"decade\"\n",
    "        }\n",
    "\n",
    "    # Single year (fallback to your extract_date_from_text)\n",
    "    d = extract_date_from_text(text or \"\")\n",
    "    if d:\n",
    "        # If d is '1990s' we already handled above; here treat numeric year\n",
    "        ym = re.search(YEAR, d)\n",
    "        if ym:\n",
    "            y = int(ym.group(0))\n",
    "            return {\n",
    "                \"start_year\": y, \"end_year\": y,\n",
    "                \"start_month\": None, \"end_month\": None,\n",
    "                \"granularity\": \"year\"\n",
    "            }\n",
    "\n",
    "    # Nothing found\n",
    "    return {\n",
    "        \"start_year\": None, \"end_year\": None,\n",
    "        \"start_month\": None, \"end_month\": None,\n",
    "        \"granularity\": None\n",
    "    }\n",
    "\n",
    "def anchor_strength(sty: int, eny: int, stm: int=None, enm: int=None, granularity: str=None) -> float:\n",
    "    \"\"\"\n",
    "    Simple anchor quality heuristic:\n",
    "    month > single-year > range (short) > decade > unknown\n",
    "    \"\"\"\n",
    "    if sty is None:\n",
    "        return 0.0\n",
    "    if granularity == \"month\" and stm:\n",
    "        return 1.0\n",
    "    if granularity == \"year\":\n",
    "        return 0.9\n",
    "    if granularity == \"range\":\n",
    "        # shorter range -> stronger\n",
    "        if (sty is not None) and (eny is not None):\n",
    "            span = max(1, eny - sty)\n",
    "            return max(0.75, 0.95 - 0.02 * span)\n",
    "        return 0.8\n",
    "    if granularity == \"decade\":\n",
    "        return 0.7\n",
    "    return 0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "811ea1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EntityRuler for label/org terms\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "entity_patterns = []\n",
    "\n",
    "# Common organization suffixes (Records, Music, Label, Inc., Ltd., GmbH, etc.)\n",
    "org_suffix_words = [\n",
    "    \"Records\",\"Recordings\",\"Music\",\"Label\",\"Studios\",\"Studio\",\"Company\",\"Co.\",\"Enterprises\"\n",
    "]\n",
    "legal_suffix_words = [\"Inc.\",\"Inc\",\"Ltd.\",\"Ltd\",\"GmbH\",\"S.R.L.\",\"SRL\",\"S.A.\",\"SA\",\"BV\",\"LLC\"]\n",
    "\n",
    "for suf in org_suffix_words + legal_suffix_words:\n",
    "    entity_patterns.append({\n",
    "        \"label\": \"ORG\",\n",
    "        \"pattern\": [{\"IS_TITLE\": True, \"OP\": \"+\"}, {\"TEXT\": suf}]\n",
    "    })\n",
    "\n",
    "# Alias markers (aka / also known as) for later rename extraction\n",
    "entity_patterns += [\n",
    "    {\"label\": \"ALIAS_MARK\", \"pattern\": [{\"LOWER\": {\"IN\": [\"aka\",\"a.k.a.\",\"a.k.a\"]}}]},\n",
    "    {\"label\": \"ALIAS_MARK\", \"pattern\": [{\"LOWER\": \"also\"}, {\"LOWER\": \"known\"}, {\"LOWER\": \"as\"}]},\n",
    "]\n",
    "\n",
    "# A few high-precision label keywords (as ORG)\n",
    "entity_patterns += [\n",
    "    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": {\"IN\": [\"label\",\"imprint\"]}}]},\n",
    "    {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": {\"IN\": [\"record\",\"records\",\"recordings\",\"music\"]}}, {\"LOWER\": \"label\"}]}\n",
    "]\n",
    "\n",
    "ruler.add_patterns(entity_patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48808cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.event_extractor(doc)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Event extraction pipeline\n",
    "# spaCy + rules：Founded / Merged / Discontinued / Rename (+ aka)\n",
    "\n",
    "# Time extraction\n",
    "# from xxxx to xxxx / xxxx–xxxx\n",
    "RANGE_PATTERNS = [\n",
    "    rf\"\\bfrom\\s+({YEAR})\\s*(?:–|-|to)\\s*(?:{YEAR}|xxxx)\\b\",\n",
    "    rf\"\\b({YEAR})\\s*(?:–|-|to)\\s*(?:{YEAR}|xxxx)\\b\",\n",
    "]\n",
    "SINCE_PATTERN = rf\"\\bsince\\s+(?:the\\s+)?({YEAR_OR_DECADE})\\b\"\n",
    "\n",
    "def extract_date_from_text(text: str) -> str:\n",
    "    m = re.search(rf\"\\b{YEAR_OR_DECADE}\\b\", text, flags=re.IGNORECASE)\n",
    "    return (m.group(0) if m else \"\")\n",
    "\n",
    "def extract_range_from_text(text: str):\n",
    "    t = text\n",
    "    for pat in RANGE_PATTERNS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            start = m.group(1)\n",
    "            m2 = re.search(rf\"(?:–|-|to)\\s*((?:{YEAR})|xxxx)\\b\", t[m.end(1):], flags=re.IGNORECASE)\n",
    "            end = None\n",
    "            if m2:\n",
    "                end_txt = m2.group(1)\n",
    "                end = end_txt if end_txt.lower() != \"xxxx\" else None\n",
    "            return start, end\n",
    "    # since\n",
    "    m = re.search(SINCE_PATTERN, t, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return m.group(1), None\n",
    "    return None, None\n",
    "\n",
    "# Rename extraction\n",
    "# proper nouns/nouns/words with capitalized first letters + several connecting words (of/the/and/records/music/label/recordings)\n",
    "def extract_proper_name_after(span):\n",
    "    sent = span.sent\n",
    "    tokens = list(sent)\n",
    "    start_i = span.end - sent.start\n",
    "\n",
    "    out = []\n",
    "    for tok in tokens[start_i:]:\n",
    "        if tok.is_space:\n",
    "            continue\n",
    "        if tok.is_punct and tok.text not in [\"&\", \"-\", \".\", \"'\", \"/\", \"’\"]:\n",
    "            break\n",
    "        if tok.pos_ in (\"PROPN\", \"NOUN\") or re.match(r\"[A-Z][\\w\\.\\-&'’/]+$\", tok.text):\n",
    "            out.append(tok.text)\n",
    "            continue\n",
    "        if out and tok.lower_ in {\"of\", \"the\", \"and\", \"records\", \"music\", \"label\", \"recordings\"}:\n",
    "            out.append(tok.text)\n",
    "            continue\n",
    "        if out:\n",
    "            break\n",
    "\n",
    "    new_name = \" \".join(out).strip()\n",
    "    new_name = re.sub(r\"^[\\(\\[]|[\\)\\]]$\", \"\", new_name).strip()\n",
    "    new_name = re.sub(r\"\\s*\\[\\d+\\]\\s*$\", \"\", new_name).strip()\n",
    "    return new_name\n",
    "\n",
    "\n",
    "Doc.set_extension(\"events\", default=[], force=True)\n",
    "\n",
    "\n",
    "# Triggers(phase/lemma)\n",
    "PHRASE_TRIGGERS = {\n",
    "    \"Founded\": [\n",
    "        \"set up\",\"came into existence\",\"began operations\",\"first appeared\",\n",
    "        \"came about\",\"was set up\",\"got started\",\"founded\",\"established\",\"launched\"\n",
    "    ],\n",
    "    \"Merged\": [\n",
    "        \"joined forces with\",\"taken over\",\"bought by\",\"was folded into\",\n",
    "        \"consolidated into\",\"combined with\",\"blended into\",\"partnered with\",\n",
    "        \"was integrated into\",\"acquired by\"\n",
    "    ],\n",
    "    \"Discontinued\": [\n",
    "        \"shut down\",\"ceased operations\",\"no longer active\",\"ceased to exist\",\n",
    "        \"ceased label operations\",\"ceased all activity\",\"wrapped up\",\"was made defunct\",\"put to rest\"\n",
    "    ],\n",
    "    \"Rename\": [\n",
    "        \"renamed to\",\"name changed to\",\"rebranded as\",\"now known as\",\n",
    "        \"changed its name to\",\"adopted the name\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "LEMMA_TRIGGERS = {\n",
    "    \"Founded\": [\"found\",\"establish\",\"start\",\"create\",\"form\",\"launch\",\"initiate\",\"inaugurate\",\"originate\",\"kickstart\",\"build\",\"born\",\"release\"],\n",
    "    \"Merged\": [\"merge\",\"acquire\",\"absorb\",\"purchase\"],\n",
    "    \"Discontinued\": [\"discontinue\",\"fold\",\"close\",\"retire\",\"disband\",\"sell\",\"defunct\"],\n",
    "    \"Rename\": [\"rename\",\"rebrand\",\"retitle\",\"redesignate\"],\n",
    "}\n",
    "\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "for evt_type, phrases in PHRASE_TRIGGERS.items():\n",
    "    patterns = [nlp.make_doc(p) for p in phrases]\n",
    "    phrase_matcher.add(evt_type.upper(), patterns)\n",
    "\n",
    "# aka / also known as / alias\n",
    "AKA_REGEX = re.compile(\n",
    "    r\"\\b(a\\.?k\\.?a\\.?|aka|also\\s+known\\s+as|alias)\\b\\s*[:\\-]?\\s*(?P<new>[\\(\\[]?[A-Z][\\w\\.\\-&' /]{2,}[\\)\\]]?)\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def _dedup(events):\n",
    "    # Event deduplication\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for e in events:\n",
    "        key = tuple(sorted(e.items()))\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            out.append(e)\n",
    "    return out\n",
    "\n",
    "# event_extractor\n",
    "@Language.component(\"event_extractor\")\n",
    "def event_extractor(doc):\n",
    "    events = []\n",
    "\n",
    "    # First scan \"range/since\" → directly output Founded(start) / Discontinued(end)\n",
    "    for sent in doc.sents:\n",
    "        start, end = extract_range_from_text(sent.text)\n",
    "        if start:\n",
    "            events.append({\n",
    "                \"event_type\": \"Founded\",\n",
    "                \"trigger\": f\"range:{sent.text}\",\n",
    "                \"time\": start\n",
    "            })\n",
    "            if end:\n",
    "                events.append({\n",
    "                    \"event_type\": \"Discontinued\",\n",
    "                    \"trigger\": f\"range:{sent.text}\",\n",
    "                    \"time\": end\n",
    "                })\n",
    "\n",
    "    # phrase matching\n",
    "    matches = phrase_matcher(doc)\n",
    "    for match_id, start_i, end_i in matches:\n",
    "        span = doc[start_i:end_i]\n",
    "        sent = span.sent\n",
    "        label = nlp.vocab.strings[match_id].capitalize()\n",
    "\n",
    "        # Non-Rename events must have time\n",
    "        date = \"\"\n",
    "        if label != \"Rename\":\n",
    "            # 1) DATE entity\n",
    "            for ent in sent.ents:\n",
    "                if ent.label_ == \"DATE\":\n",
    "                    date = ent.text\n",
    "                    break\n",
    "            # 2) Text year/decade\n",
    "            if not date:\n",
    "                date = extract_date_from_text(sent.text)\n",
    "            # 3) Range starting point\n",
    "            if not date:\n",
    "                s, _ = extract_range_from_text(sent.text)\n",
    "                if s:\n",
    "                    date = s\n",
    "            if not date:\n",
    "                continue\n",
    "\n",
    "        ev = {\"event_type\": label, \"trigger\": span.text, \"time\": date}\n",
    "        if label == \"Rename\":\n",
    "            new_name = extract_proper_name_after(span)\n",
    "            if new_name:\n",
    "                ev[\"new_name\"] = new_name\n",
    "        events.append(ev)\n",
    "\n",
    "    # lemma matching\n",
    "    for sent in doc.sents:\n",
    "        lemmas = [t.lemma_.lower() for t in sent]\n",
    "        for evt_type, trig_list in LEMMA_TRIGGERS.items():\n",
    "            for trig in trig_list:\n",
    "                if trig in lemmas:\n",
    "                    date = \"\"\n",
    "                    if evt_type != \"Rename\":\n",
    "                        for ent in sent.ents:\n",
    "                            if ent.label_ == \"DATE\":\n",
    "                                date = ent.text\n",
    "                                break\n",
    "                        if not date:\n",
    "                            date = extract_date_from_text(sent.text)\n",
    "                        if not date:\n",
    "                            s, _ = extract_range_from_text(sent.text)\n",
    "                            if s:\n",
    "                                date = s\n",
    "                        if not date:\n",
    "                            continue\n",
    "                    ev = {\"event_type\": evt_type, \"trigger\": trig, \"time\": date}\n",
    "                    if evt_type == \"Rename\":\n",
    "                        try:\n",
    "                            idx = lemmas.index(trig)\n",
    "                            span_like = sent[idx: idx+2]\n",
    "                            new_name = extract_proper_name_after(span_like)\n",
    "                            if new_name:\n",
    "                                ev[\"new_name\"] = new_name\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    events.append(ev)\n",
    "\n",
    "    # aka / also known as / alias\n",
    "    for sent in doc.sents:\n",
    "        for m in AKA_REGEX.finditer(sent.text):\n",
    "            new_name = m.group(\"new\").strip(\"()[] \").strip()\n",
    "            new_name = re.sub(r\"\\s*\\[\\d+\\]\\s*$\", \"\", new_name).strip()\n",
    "            date = extract_date_from_text(sent.text)\n",
    "            if not date:\n",
    "                s, _ = extract_range_from_text(sent.text)\n",
    "                if s:\n",
    "                    date = s\n",
    "            events.append({\n",
    "                \"event_type\": \"Rename\",\n",
    "                \"trigger\": m.group(0),\n",
    "                \"time\": date,\n",
    "                \"new_name\": new_name\n",
    "            })\n",
    "\n",
    "    # decade fallback (in/early/late the 1960s etc.)\n",
    "    for sent in doc.sents:\n",
    "        m = re.search(rf\"\\b(in|since|from|around|during|early|late)?\\s*(the\\s+)?({DECADE})\\b\", sent.text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            decade = re.search(DECADE, m.group(0), flags=re.IGNORECASE).group(0)\n",
    "            if not any(decade == (e.get(\"time\") or \"\") for e in events):\n",
    "                events.append({\n",
    "                    \"event_type\": \"Founded\",\n",
    "                    \"trigger\": m.group(0),\n",
    "                    \"time\": decade\n",
    "                })\n",
    "\n",
    "    doc._.events = _dedup(events)\n",
    "    return doc\n",
    "\n",
    "# Add component to pipeline (after ner)\n",
    "nlp.add_pipe(\"event_extractor\", after=\"ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3445e505-76e0-4ce6-8690-452c5bc45e8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "3445e505-76e0-4ce6-8690-452c5bc45e8b",
    "outputId": "9df797e8-e897-4c34-ec3c-d0c5c201ba23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.relation_extractor(doc)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register custom Doc extension for storing relations\n",
    "Doc.set_extension(\"relations\", default=[], force=True)\n",
    "\n",
    "# Define relation extraction pipeline component\n",
    "@Language.component(\"relation_extractor\")\n",
    "def relation_extractor(doc):\n",
    "    relations = []\n",
    "    for sent in doc.sents:\n",
    "        subject = None\n",
    "        verb = None\n",
    "        object_ = None\n",
    "        for token in sent:\n",
    "            if \"subj\" in token.dep_:\n",
    "                subject = token.text\n",
    "            if \"obj\" in token.dep_:\n",
    "                object_ = token.text\n",
    "            if token.pos_ == \"VERB\":\n",
    "                verb = token.lemma_\n",
    "        if subject and verb and object_:\n",
    "            relations.append({\"subject\": subject, \"verb\": verb, \"object\": object_, \"sentence\": sent.text})\n",
    "    doc._.relations = relations\n",
    "    return doc\n",
    "\n",
    "# Add relation extractor after event extractor\n",
    "nlp.add_pipe(\"relation_extractor\", after=\"event_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c39e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.relation_extractor_patched(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- SVO cache patched extractor (keeps your original logic untouched) ---\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"_relations_cache\", default=[], force=True)\n",
    "\n",
    "@Language.component(\"relation_extractor_patched\")\n",
    "def relation_extractor_patched(doc: Doc) -> Doc:\n",
    "    \"\"\"\n",
    "    Store SVO tuples per sentence so later components (regex matcher / timeline)\n",
    "    can boost confidence when SVO aligns with triggers.\n",
    "    \"\"\"\n",
    "    cache = []\n",
    "    for sent in doc.sents:\n",
    "        subject = verb = object_ = None\n",
    "        for tok in sent:\n",
    "            if \"subj\" in tok.dep_:\n",
    "                subject = tok.text\n",
    "            if \"obj\" in tok.dep_:\n",
    "                object_ = tok.text\n",
    "            if tok.pos_ == \"VERB\":\n",
    "                verb = tok.lemma_.lower()\n",
    "        if subject and verb and object_:\n",
    "            cache.append({\n",
    "                \"sentence\": sent.text,\n",
    "                \"subject\": subject,\n",
    "                \"verb\": verb,\n",
    "                \"object\": object_\n",
    "            })\n",
    "    doc._._relations_cache = cache\n",
    "    return doc\n",
    "\n",
    "# Register into pipeline right after your event_extractor\n",
    "nlp.add_pipe(\"relation_extractor_patched\", after=\"event_extractor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "843bdeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.regex_relation_matcher(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Regex relation matcher (high-precision patterns) ---\n",
    "import re\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "Doc.set_extension(\"regex_relations\", default=[], force=True)\n",
    "\n",
    "# Reuse your YEAR/DECADE helpers and extract_date_from_text/extract_range_from_text if already defined.\n",
    "# Mini trigger map for event types:\n",
    "_RE_TRIG = {\n",
    "    \"Founded\": re.compile(r\"\\b(founded|established|set\\s+up|formed|created)\\b\", re.I),\n",
    "    \"Merged\": re.compile(r\"\\b(merged\\s+with|acquired|absorbed|took\\s+over)\\b\", re.I),\n",
    "    \"Discontinued\": re.compile(r\"\\b(discontinued|folded|ceased\\s+operations|shut\\s+down)\\b\", re.I),\n",
    "    \"Rename\": re.compile(r\"\\b(renamed|rebranded\\s+as|changed\\s+name\\s+to)\\b\", re.I),\n",
    "}\n",
    "\n",
    "@Language.component(\"regex_relation_matcher\")\n",
    "def regex_relation_matcher(doc: Doc) -> Doc:\n",
    "    \"\"\"\n",
    "    Create extra event candidates from a tiny set of precise regex patterns,\n",
    "    complementing your rule/phrase triggers.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for sent in doc.sents:\n",
    "        s = sent.text\n",
    "        etype = None\n",
    "        for k, pat in _RE_TRIG.items():\n",
    "            if pat.search(s):\n",
    "                etype = k\n",
    "                break\n",
    "        if not etype:\n",
    "            continue\n",
    "\n",
    "        # Get a time string using your existing helpers (safe if missing)\n",
    "        t = \"\"\n",
    "        s_start, s_end = extract_range_from_text(s)\n",
    "        if s_start:\n",
    "            t = s_start\n",
    "        if not t:\n",
    "            t = extract_date_from_text(s)\n",
    "\n",
    "        cand = {\n",
    "            \"event_type\": etype,\n",
    "            \"trigger\": pat.search(s).group(0) if _RE_TRIG[etype].search(s) else \"\",\n",
    "            \"time\": t,\n",
    "            \"sentence\": s\n",
    "        }\n",
    "        # Try to capture new_name for Rename\n",
    "        if etype == \"Rename\":\n",
    "            cand[\"new_name\"] = extract_proper_name_after(sent) if 'extract_proper_name_after' in globals() else None\n",
    "\n",
    "        out.append(cand)\n",
    "\n",
    "    doc._.regex_relations = out\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"regex_relation_matcher\", after=\"relation_extractor_patched\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a77170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.timeline_builder(doc: spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Timeline builder with SVO boosting and regex merge\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Keep your original normalize_time() and anchor_strength() intact.\n",
    "\n",
    "Doc.set_extension(\"timeline\", default=[], force=True)\n",
    "\n",
    "# Map event types to acceptable verb lemmas for SVO alignment\n",
    "_EVT_VERBS = {\n",
    "    \"Founded\": {\"found\",\"establish\",\"set\",\"form\",\"create\",\"launch\",\"start\"},\n",
    "    \"Merged\": {\"merge\",\"acquire\",\"absorb\",\"purchase\",\"take\"},\n",
    "    \"Discontinued\": {\"discontinue\",\"fold\",\"close\",\"shut\",\"cease\"},\n",
    "    \"Rename\": {\"rename\",\"rebrand\",\"retitle\"}\n",
    "}\n",
    "\n",
    "@Language.component(\"timeline_builder\")\n",
    "def timeline_builder(doc: Doc) -> Doc:\n",
    "    \"\"\"\n",
    "    Merge events from:\n",
    "      - doc._.events (your existing extractor)\n",
    "      - doc._.regex_relations (the new regex layer)\n",
    "    Normalize time, compute confidence with anchor_strength, and boost if SVO aligns.\n",
    "    \"\"\"\n",
    "    cands = []\n",
    "\n",
    "    # From your existing events\n",
    "    if hasattr(doc._, \"events\") and doc._.events:\n",
    "        for ev in doc._.events:\n",
    "            t = ev.get(\"time\", \"\")\n",
    "            norm = normalize_time(str(t) if t is not None else \"\")\n",
    "            sty, eny = norm[\"start_year\"], norm[\"end_year\"]\n",
    "            stm, enm = norm[\"start_month\"], norm[\"end_month\"]\n",
    "            gran = norm[\"granularity\"]\n",
    "            conf = anchor_strength(sty, eny, stm, enm, gran)\n",
    "            if ev.get(\"trigger\"):\n",
    "                conf = min(1.0, conf + 0.05)\n",
    "            cands.append({\n",
    "                \"event_type\": ev.get(\"event_type\"),\n",
    "                \"trigger\": ev.get(\"trigger\"),\n",
    "                \"new_name\": ev.get(\"new_name\"),\n",
    "                \"raw_time\": t,\n",
    "                \"start_year\": sty, \"end_year\": eny,\n",
    "                \"start_month\": stm, \"end_month\": enm,\n",
    "                \"granularity\": gran,\n",
    "                \"confidence\": conf,\n",
    "                \"sentence\": ev.get(\"sentence\",\"\")\n",
    "            })\n",
    "\n",
    "    # From regex layer\n",
    "    if hasattr(doc._, \"regex_relations\") and doc._.regex_relations:\n",
    "        for ev in doc._.regex_relations:\n",
    "            t = ev.get(\"time\",\"\")\n",
    "            norm = normalize_time(str(t) if t is not None else \"\")\n",
    "            sty, eny = norm[\"start_year\"], norm[\"end_year\"]\n",
    "            stm, enm = norm[\"start_month\"], norm[\"end_month\"]\n",
    "            gran = norm[\"granularity\"]\n",
    "            conf = anchor_strength(sty, eny, stm, enm, gran) + 0.1  # slight base bonus for precise regex\n",
    "            cands.append({\n",
    "                \"event_type\": ev.get(\"event_type\"),\n",
    "                \"trigger\": ev.get(\"trigger\"),\n",
    "                \"new_name\": ev.get(\"new_name\"),\n",
    "                \"raw_time\": t,\n",
    "                \"start_year\": sty, \"end_year\": eny,\n",
    "                \"start_month\": stm, \"end_month\": enm,\n",
    "                \"granularity\": gran,\n",
    "                \"confidence\": min(1.0, conf),\n",
    "                \"sentence\": ev.get(\"sentence\",\"\")\n",
    "            })\n",
    "\n",
    "    # SVO alignment boost (+0.25) if sentence’s verb lemma matches event type verbs\n",
    "    cache = getattr(doc._, \"_relations_cache\", [])\n",
    "    for c in cands:\n",
    "        sent_text = c.get(\"sentence\") or \"\"\n",
    "        if not sent_text and c.get(\"trigger\"):\n",
    "            # Heuristic: find the sentence containing trigger\n",
    "            for s in doc.sents:\n",
    "                if c[\"trigger\"] and c[\"trigger\"] in s.text:\n",
    "                    sent_text = s.text\n",
    "                    break\n",
    "            c[\"sentence\"] = sent_text\n",
    "        if not sent_text:\n",
    "            continue\n",
    "        # find SVO for this sentence\n",
    "        for item in cache:\n",
    "            if item[\"sentence\"] == sent_text:\n",
    "                verb = item[\"verb\"]\n",
    "                if verb in _EVT_VERBS.get(c[\"event_type\"], set()):\n",
    "                    c[\"confidence\"] = min(1.0, c[\"confidence\"] + 0.25)\n",
    "                break\n",
    "\n",
    "    # Conflict resolution: keep best by (event_type, sentence or trigger)\n",
    "    best = {}\n",
    "    for c in cands:\n",
    "        key = (c[\"event_type\"], c.get(\"sentence\") or c.get(\"trigger\"))\n",
    "        if key not in best or c[\"confidence\"] > best[key][\"confidence\"]:\n",
    "            best[key] = c\n",
    "\n",
    "    tl = list(best.values())\n",
    "\n",
    "    # Sort by normalized time then confidence\n",
    "    def _k(x):\n",
    "        y = x[\"start_year\"] if x[\"start_year\"] is not None else 99999\n",
    "        m = x[\"start_month\"] if x[\"start_month\"] is not None else 12\n",
    "        return (y, m, -x[\"confidence\"])\n",
    "    tl.sort(key=_k)\n",
    "\n",
    "    doc._.timeline = tl\n",
    "    return doc\n",
    "\n",
    "# Re-register after regex matcher\n",
    "nlp.add_pipe(\"timeline_builder\", after=\"regex_relation_matcher\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098c1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → /workspace/sample_for_annotation.xlsx | rows=20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>profile</th>\n",
       "      <th>true_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prima Immergrün</td>\n",
       "      <td>Series created by German budget label [l=Prima...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Archive Of Piano Music (WRC)</td>\n",
       "      <td>24-LP series of historic \"recordings\" from the...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attytude Records</td>\n",
       "      <td>[a776]'s label created in 2019.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fresno Four</td>\n",
       "      <td>USA label created for [a4438775].</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ionic Records (2)</td>\n",
       "      <td>1960s rock label from Hollywood, California.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vinyllover Recordings</td>\n",
       "      <td>Label for electronic dance music founded in 20...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ena Music Production</td>\n",
       "      <td>Slovakian Eurodance Label\\r\\nRenamed to [l=Ena...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Huvila Mielenrauha</td>\n",
       "      <td>A villa in Ivalo, Lapland, Finland where [a=Ve...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>princefansunited.com</td>\n",
       "      <td>In November 2007 an ephemeral protest campaign...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Les Découvertes</td>\n",
       "      <td>Series of promotional CDs, showcasing newcomer...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bonton</td>\n",
       "      <td>Czech and Slovak music label. Established in 1...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Studio Frémontel</td>\n",
       "      <td>Was known before as [l334205]. Started in 1971...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Frontline Music Group</td>\n",
       "      <td>Frontline Music Group was a California-based C...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multitone (2)</td>\n",
       "      <td>UK label with bhangra, quawwali and Indian mus...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gothic Print Finishers Ltd.</td>\n",
       "      <td>British printing company founded on 15 June 19...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10x Entertainment Corporation</td>\n",
       "      <td>South Korean entertainment company based in Se...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Moondays Srl</td>\n",
       "      <td>Moondays is an Italian highly imaginative and ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Capitol Records-EMI Of Canada Limited</td>\n",
       "      <td>Canadian branch of Capitol Records and EMI. Pr...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rodeheaver Publishing Co.</td>\n",
       "      <td>Publishing company started by [a=Homer Rodehea...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WEA Music K.K.</td>\n",
       "      <td>Japanese record company.\\r\\n[b]ＷＥＡ　ミュージック　株式会社...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name  \\\n",
       "0                         Prima Immergrün   \n",
       "1            Archive Of Piano Music (WRC)   \n",
       "2                        Attytude Records   \n",
       "3                             Fresno Four   \n",
       "4                       Ionic Records (2)   \n",
       "5                   Vinyllover Recordings   \n",
       "6                    Ena Music Production   \n",
       "7                      Huvila Mielenrauha   \n",
       "8                    princefansunited.com   \n",
       "9                         Les Découvertes   \n",
       "10                                 Bonton   \n",
       "11                       Studio Frémontel   \n",
       "12                  Frontline Music Group   \n",
       "13                          Multitone (2)   \n",
       "14            Gothic Print Finishers Ltd.   \n",
       "15          10x Entertainment Corporation   \n",
       "16                           Moondays Srl   \n",
       "17  Capitol Records-EMI Of Canada Limited   \n",
       "18              Rodeheaver Publishing Co.   \n",
       "19                         WEA Music K.K.   \n",
       "\n",
       "                                              profile true_events  \n",
       "0   Series created by German budget label [l=Prima...          []  \n",
       "1   24-LP series of historic \"recordings\" from the...          []  \n",
       "2                     [a776]'s label created in 2019.          []  \n",
       "3                   USA label created for [a4438775].          []  \n",
       "4        1960s rock label from Hollywood, California.          []  \n",
       "5   Label for electronic dance music founded in 20...          []  \n",
       "6   Slovakian Eurodance Label\\r\\nRenamed to [l=Ena...          []  \n",
       "7   A villa in Ivalo, Lapland, Finland where [a=Ve...          []  \n",
       "8   In November 2007 an ephemeral protest campaign...          []  \n",
       "9   Series of promotional CDs, showcasing newcomer...          []  \n",
       "10  Czech and Slovak music label. Established in 1...          []  \n",
       "11  Was known before as [l334205]. Started in 1971...          []  \n",
       "12  Frontline Music Group was a California-based C...          []  \n",
       "13  UK label with bhangra, quawwali and Indian mus...          []  \n",
       "14  British printing company founded on 15 June 19...          []  \n",
       "15  South Korean entertainment company based in Se...          []  \n",
       "16  Moondays is an Italian highly imaginative and ...          []  \n",
       "17  Canadian branch of Capitol Records and EMI. Pr...          []  \n",
       "18  Publishing company started by [a=Homer Rodehea...          []  \n",
       "19  Japanese record company.\\r\\n[b]ＷＥＡ　ミュージック　株式会社...          []  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Export a single review file with columns: [name, profile, true_events].\n",
    "# All comments are in English as requested.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def export_review_minimal(\n",
    "    input_file: str,\n",
    "    output_file: str = \"review_minimal.xlsx\",\n",
    "    type_col: str = \"event_type\",\n",
    "    name_col: str = \"name\",\n",
    "    profile_col: str = \"profile\",\n",
    "    k_per_class: int = 5,\n",
    "    seed: int = 42,\n",
    "    drop_empty_profile: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load CSV/Excel, sample exactly k_per_class rows per event type (take all if fewer),\n",
    "    and export a minimal sheet with only [name, profile, true_events].\n",
    "    The 'true_events' column is initialized as \"[]\", to be filled manually as a JSON list.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Load source table\n",
    "    suffix = Path(input_file).suffix.lower()\n",
    "    if suffix == \".csv\":\n",
    "        df = pd.read_csv(input_file)\n",
    "    else:\n",
    "        df = pd.read_excel(input_file)\n",
    "\n",
    "    # 2) Sanity checks\n",
    "    for c in [type_col, name_col, profile_col]:\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Column '{c}' not found in input data.\")\n",
    "\n",
    "    # 3) Optional: filter out empty profiles\n",
    "    data = df.copy()\n",
    "    if drop_empty_profile:\n",
    "        data = data[data[profile_col].astype(str).str.strip() != \"\"].copy()\n",
    "\n",
    "    # 4) Stratified sampling: exactly k per class when possible\n",
    "    rng = np.random.default_rng(seed)\n",
    "    parts = []\n",
    "    for etype, g in data.groupby(type_col, sort=False):\n",
    "        if len(g) <= k_per_class:\n",
    "            parts.append(g)\n",
    "        else:\n",
    "            parts.append(g.sample(n=k_per_class, random_state=int(rng.integers(0, 1_000_000))))\n",
    "    sampled = pd.concat(parts, axis=0).reset_index(drop=True)\n",
    "\n",
    "    # 5) Build minimal schema\n",
    "    out = sampled[[name_col, profile_col]].copy()\n",
    "    out.rename(columns={name_col: \"name\", profile_col: \"profile\"}, inplace=True)\n",
    "    out[\"true_events\"] = \"[]\"  # You will fill JSON array, e.g., [{\"time\":\"1958-01-31\"}, {\"time\":\"1974\"}]\n",
    "\n",
    "    # 6) Export a SINGLE file (Excel or CSV based on extension)\n",
    "    if output_file.lower().endswith(\".csv\"):\n",
    "        out.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "    else:\n",
    "        out.to_excel(output_file, index=False)\n",
    "\n",
    "    # 7) Brief report\n",
    "    print(f\"Saved → {output_file} | rows={len(out)}\")\n",
    "    return out\n",
    "\n",
    "export_review_minimal(\n",
    "     input_file=\"/workspace/timeline_events.csv\", \n",
    "     output_file=\"/workspace/sample_for_annotation.xlsx\",\n",
    "     type_col=\"event_type\",\n",
    "     name_col=\"label_name\",\n",
    "     profile_col=\"profile\",\n",
    "     k_per_class=5,\n",
    "     seed=20250818\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38820246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated data prepared, total20 records\n",
      "                           name  \\\n",
      "0               Prima Immergrün   \n",
      "1  Archive Of Piano Music (WRC)   \n",
      "\n",
      "                                             profile  \\\n",
      "0  Series created by German budget label [l=Prima...   \n",
      "1  24-LP series of historic \"recordings\" from the...   \n",
      "\n",
      "                                         true_events  \\\n",
      "0  [{\"event_type\": \"Founded\", \"time\": \"\", \"trigge...   \n",
      "1  [{\"event_type\": \"Founded\", \"time\": \"early 20th...   \n",
      "\n",
      "                                    predicted_events  \n",
      "0                                                 []  \n",
      "1  [{\"event_type\": \"Founded\", \"trigger\": \"form\", ...  \n",
      "True Events freq: Counter({'Founded': 15, 'Discontinued': 5, 'Merged': 5, 'Rename': 4})\n",
      "Pred Events freq: Counter({'Founded': 11, 'Rename': 6, 'Discontinued': 6, 'Merged': 5})\n",
      "\n",
      "Per-class metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Discontinued       0.67      0.80      0.73         5\n",
      "     Founded       1.00      0.73      0.85        15\n",
      "      Merged       1.00      1.00      1.00         5\n",
      "      Rename       0.67      1.00      0.80         4\n",
      "\n",
      "   micro avg       0.86      0.83      0.84        29\n",
      "   macro avg       0.83      0.88      0.84        29\n",
      "weighted avg       0.90      0.83      0.85        29\n",
      " samples avg       0.72      0.78      0.73        29\n",
      "\n",
      "Overall accuracy 0.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, ast, re, difflib\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "ANNOTATION_FILE = \"/workspace/sample_for_annotation_annotated.xlsx\"  # Manual annotations\n",
    "TO_PREDICT_FILE = \"/workspace/sample_for_annotation.xlsx\"           # Data to predict\n",
    "\n",
    "# Read manual annotations\n",
    "gold_df = pd.read_excel(ANNOTATION_FILE)\n",
    "assert {\"name\",\"profile\",\"true_events\"}.issubset(gold_df.columns)\n",
    "\n",
    "# Read data to predict and run pipeline\n",
    "pred_df = pd.read_excel(TO_PREDICT_FILE)\n",
    "assert {\"name\",\"profile\"}.issubset(pred_df.columns)\n",
    "\n",
    "def run_pipeline(text):\n",
    "    doc = nlp(text if isinstance(text, str) else \"\")\n",
    "    return json.dumps(doc._.events, ensure_ascii=False)\n",
    "\n",
    "pred_df[\"predicted_events\"] = pred_df[\"profile\"].apply(run_pipeline)\n",
    "\n",
    "# Merge datasets\n",
    "annotated = gold_df[[\"name\",\"profile\",\"true_events\"]].merge(\n",
    "    pred_df[[\"name\",\"predicted_events\"]], on=\"name\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Fill NaN values\n",
    "annotated[\"true_events\"] = annotated[\"true_events\"].fillna(\"[]\")\n",
    "annotated[\"predicted_events\"] = annotated[\"predicted_events\"].fillna(\"[]\")\n",
    "\n",
    "print(f\"annotated data prepared, total{len(annotated)} records\")\n",
    "print(annotated.head(2))\n",
    "\n",
    "SIM_TRIG = 0.6 # Similarity threshold for trigger matching\n",
    "ALLOW_EMPTY_TIME_MATCH = True # Allow events to match even if one has empty time\n",
    "\n",
    "# Normalize event data to a consistent format.\n",
    "def normalize_event_list(event_data):\n",
    "    if isinstance(event_data, str):\n",
    "        try:\n",
    "            event_data = ast.literal_eval(event_data)\n",
    "        except Exception:\n",
    "            return []\n",
    "    if isinstance(event_data, list):\n",
    "        if all(isinstance(e, str) for e in event_data):\n",
    "            return [{\"event_type\": e, \"time\": \"\", \"trigger\": \"\"} for e in event_data]\n",
    "        elif all(isinstance(e, dict) for e in event_data):\n",
    "            return event_data\n",
    "    return []\n",
    "\n",
    "def fuzzy_match_trigger(t_pred, t_true, thr=SIM_TRIG):\n",
    "    if not t_pred or not t_true:\n",
    "        return False\n",
    "    return difflib.SequenceMatcher(None, str(t_pred).lower(), str(t_true).lower()).ratio() >= thr\n",
    "\n",
    "def norm_time(t):\n",
    "    if not t:\n",
    "        return (\"\", \"\")\n",
    "    t = str(t)\n",
    "    m_year = re.search(r'\\b(19|20)\\d{2}\\b', t)\n",
    "    if m_year:\n",
    "        return (\"year\", m_year.group(0))\n",
    "    m_dec1 = re.search(r'\\b(19|20)\\d0s\\b', t)\n",
    "    if m_dec1:\n",
    "        return (\"decade\", m_dec1.group(0))\n",
    "    m_dec2 = re.search(r'\\b([5-9]0)s\\b', t)\n",
    "    if m_dec2:\n",
    "        return (\"decade_short\", m_dec2.group(0))\n",
    "    return (\"raw\", t.strip())\n",
    "\n",
    "def loose_time_equal(a, b, allow_empty=ALLOW_EMPTY_TIME_MATCH):\n",
    "    if not a and not b:\n",
    "        return True\n",
    "    if not a or not b:\n",
    "        return allow_empty\n",
    "    ka, va = norm_time(a); kb, vb = norm_time(b)\n",
    "    if (ka, va) == (kb, vb):\n",
    "        return True\n",
    "    if ka == \"year\" and kb in (\"decade\", \"decade_short\"):\n",
    "        return vb.startswith(va[:3])\n",
    "    if kb == \"year\" and ka in (\"decade\", \"decade_short\"):\n",
    "        return va.startswith(vb[:3])\n",
    "    if ka == \"raw\" and kb == \"raw\":\n",
    "        return difflib.SequenceMatcher(None, va.lower(), vb.lower()).ratio() >= 0.75\n",
    "    return False\n",
    "\n",
    "\n",
    "def loose_event_equal(pred_e: dict, true_e: dict):\n",
    "    if pred_e.get(\"event_type\") != true_e.get(\"event_type\"):\n",
    "        return False\n",
    "    if not loose_time_equal(pred_e.get(\"time\", \"\"), true_e.get(\"time\", \"\")):\n",
    "        return False\n",
    "    tp, tt = pred_e.get(\"trigger\", \"\"), true_e.get(\"trigger\", \"\")\n",
    "    if tp and tt and not fuzzy_match_trigger(tp, tt):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def collapse_to_types_with_fuzzy_time_trigger(events):\n",
    "    return [e.get(\"event_type\", \"\") for e in events if isinstance(e, dict)]\n",
    "\n",
    "def match_row(pred_events, true_events):\n",
    "    used_pred = set()\n",
    "    hits, miss, spurious = [], [], []\n",
    "    for ti, t in enumerate(true_events):\n",
    "        found = False\n",
    "        for pi, p in enumerate(pred_events):\n",
    "            if pi in used_pred:\n",
    "                continue\n",
    "            if loose_event_equal(p, t):\n",
    "                hits.append((p, t))\n",
    "                used_pred.add(pi)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            miss.append(t)\n",
    "    for pi, p in enumerate(pred_events):\n",
    "        if pi not in used_pred:\n",
    "            spurious.append(p)\n",
    "    return hits, miss, spurious\n",
    "\n",
    "# Construct sets and calculate metrics\n",
    "true_sets, pred_sets, row_matches = [], [], []\n",
    "for i, row in annotated.iterrows():\n",
    "    true_events = normalize_event_list(row.get(\"true_events\", []))\n",
    "    pred_events = normalize_event_list(row.get(\"predicted_events\", []))\n",
    "    true_types = collapse_to_types_with_fuzzy_time_trigger(true_events)\n",
    "    pred_types = collapse_to_types_with_fuzzy_time_trigger(pred_events)\n",
    "    true_sets.append(set(true_types))\n",
    "    pred_sets.append(set(pred_types))\n",
    "    hits, miss, spurious = match_row(pred_events, true_events)\n",
    "    row_matches.append({\n",
    "        \"name\": row.get(\"name\", i),\n",
    "        \"profile\": row.get(\"profile\", \"\"),\n",
    "        \"hits\": hits, \"miss\": miss, \"spurious\": spurious\n",
    "    })\n",
    "\n",
    "# Output statistics\n",
    "print(\"True Events freq:\", Counter([t for s in true_sets for t in s]))\n",
    "print(\"Pred Events freq:\", Counter([t for s in pred_sets for t in s]))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(true_sets + pred_sets)\n",
    "y_true_bin = mlb.transform(true_sets)\n",
    "y_pred_bin = mlb.transform(pred_sets)\n",
    "\n",
    "print(\"\\nPer-class metrics:\")\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=mlb.classes_, zero_division=0))\n",
    "print(\"Overall accuracy\", accuracy_score(y_true_bin, y_pred_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f1544e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Time Evaluation Results\n",
      "============================================================\n",
      "Basic Statistics:\n",
      " True events total: 44\n",
      " Predicted events total: 73\n",
      " Prediction/truth ratio: 1.66\n",
      "\n",
      "Key Performance Indicators:\n",
      " Traditional precision: 0.425\n",
      " Traditional recall: 0.705\n",
      " Traditional F1 score: 0.530\n",
      " Adjusted accuracy: 0.705\n",
      "\n",
      "Time Accuracy:\n",
      " Strict time matching (exact year): 0.609\n",
      " Lenient time matching (±2 years/same decade): 0.622\n",
      "\n",
      "Time Extraction Coverage:\n",
      " True events with years: 34/44 (77.3%)\n",
      " Predicted events with years: 59/73 (80.8%)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced year extraction\n",
    "def _norm_year(x):\n",
    "    try:\n",
    "        if not x:\n",
    "            return None\n",
    "        text = str(x).strip().lower()\n",
    "        if not text:\n",
    "            return None\n",
    "\n",
    "        # Extract 4-digit years\n",
    "        year_match = re.search(r'\\b(1[8-9]\\d{2}|20[0-5]\\d)\\b', text)\n",
    "        if year_match:\n",
    "            year = int(year_match.group(0))\n",
    "            if 1800 <= year <= 2050:\n",
    "                return year\n",
    "\n",
    "        # Complex time expression handling\n",
    "        if 'early' in text and ('20th century' in text or 'twentieth century' in text):\n",
    "            return 1910  # early 20th century -> 1910\n",
    "        elif 'late' in text and ('20th century' in text or 'twentieth century' in text):\n",
    "            return 1990  # late 20th century -> 1990\n",
    "        elif '20th century' in text or 'twentieth century' in text:\n",
    "            return 1950  # 20th century -> 1950\n",
    "\n",
    "        # Standard decades (1990s -> 1995)\n",
    "        decade_match = re.search(r'\\b(1[8-9]\\d|20[0-4]\\d)0s\\b', text)\n",
    "        if decade_match:\n",
    "            base_year = int(decade_match.group(0)[:4])\n",
    "            return base_year + 5  # decade midpoint\n",
    "\n",
    "        # Short decades (80s -> 1985)\n",
    "        short_decade = re.search(r'\\b([5-9]0)s\\b', text)\n",
    "        if short_decade:\n",
    "            short_num = int(short_decade.group(1))\n",
    "            if short_num >= 50:\n",
    "                return 1900 + short_num + 5\n",
    "            else:\n",
    "                return 2000 + short_num + 5\n",
    "\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Time accuracy calculation\n",
    "def time_accuracy_per_row(true_events, pred_events):\n",
    "    \"\"\"Calculate time accuracy per row\"\"\"\n",
    "    trueY = [_norm_year(e.get(\"time\", \"\")) for e in true_events if isinstance(e, dict)]\n",
    "    predY = [_norm_year(e.get(\"time\", \"\")) for e in pred_events if isinstance(e, dict)]\n",
    "\n",
    "    if not trueY and not predY:\n",
    "        return 1.0, 1.0\n",
    "    if not trueY or not predY:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    # Use enhanced matching algorithm\n",
    "    used = set()\n",
    "    strict_hits = lenient_hits = 0\n",
    "\n",
    "    for ty in trueY:\n",
    "        best_score = -1\n",
    "        best_i = -1\n",
    "        for i, py in enumerate(predY):\n",
    "            if i in used:\n",
    "                continue\n",
    "            # Calculate matching score\n",
    "            if ty is None and py is None:\n",
    "                score = 1.0\n",
    "            elif ty is None or py is None:\n",
    "                score = 0.0\n",
    "            elif ty == py:\n",
    "                score = 1.0\n",
    "            elif abs(ty - py) <= 1:\n",
    "                score = 0.9\n",
    "            elif abs(ty - py) <= 2:\n",
    "                score = 0.8\n",
    "            elif (ty // 10 == py // 10):\n",
    "                score = 0.7\n",
    "            else:\n",
    "                score = 0.0\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_i = i\n",
    "\n",
    "        if best_i >= 0 and best_score > 0:\n",
    "            used.add(best_i)\n",
    "            if best_score >= 0.95:\n",
    "                strict_hits += 1\n",
    "                lenient_hits += 1\n",
    "            elif best_score >= 0.7:\n",
    "                lenient_hits += 1\n",
    "\n",
    "    denom = len(trueY)\n",
    "    return (strict_hits / denom if denom > 0 else 0,\n",
    "            lenient_hits / denom if denom > 0 else 0)\n",
    "\n",
    "\n",
    "# Event equality check\n",
    "def enhanced_event_equal(pred_e: dict, true_e: dict):\n",
    "    \"\"\"Enhanced event matching function\"\"\"\n",
    "    if pred_e.get(\"event_type\") != true_e.get(\"event_type\"):\n",
    "        return False\n",
    "\n",
    "    # Enhanced time matching\n",
    "    pred_time = pred_e.get(\"time\", \"\")\n",
    "    true_time = true_e.get(\"time\", \"\")\n",
    "    pred_year = _norm_year(pred_time)\n",
    "    true_year = _norm_year(true_time)\n",
    "\n",
    "    # Time matching logic\n",
    "    if pred_year is None and true_year is None:\n",
    "        time_match = True\n",
    "    elif pred_year is None or true_year is None:\n",
    "        time_match = True  # lenient\n",
    "    else:\n",
    "        time_match = (\n",
    "            pred_year == true_year or\n",
    "            abs(pred_year - true_year) <= 2 or\n",
    "            (pred_year // 10 == true_year // 10)\n",
    "        )\n",
    "\n",
    "    if not time_match:\n",
    "        return False\n",
    "\n",
    "    # Trigger matching (lenient)\n",
    "    tp, tt = pred_e.get(\"trigger\", \"\"), true_e.get(\"trigger\", \"\")\n",
    "    if tp and tt and not fuzzy_match_trigger(tp, tt, thr=0.4):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "# Match row\n",
    "def enhanced_match_row(pred_events, true_events):\n",
    "    \"\"\"Enhanced row matching function\"\"\"\n",
    "    used_pred = set()\n",
    "    hits, miss, spurious = [], [], []\n",
    "\n",
    "    for ti, t in enumerate(true_events):\n",
    "        found = False\n",
    "        for pi, p in enumerate(pred_events):\n",
    "            if pi in used_pred:\n",
    "                continue\n",
    "            if enhanced_event_equal(p, t):\n",
    "                hits.append((p, t))\n",
    "                used_pred.add(pi)\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            miss.append(t)\n",
    "\n",
    "    for pi, p in enumerate(pred_events):\n",
    "        if pi not in used_pred:\n",
    "            spurious.append(p)\n",
    "\n",
    "    return hits, miss, spurious\n",
    "\n",
    "# Evaluation Execution\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Time Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "row_matches = []\n",
    "strict_list = []\n",
    "lenient_list = []\n",
    "order_true_list = []\n",
    "order_pred_list = []\n",
    "\n",
    "for i, row in annotated.iterrows():\n",
    "    te = normalize_event_list(row.get(\"true_events\", []))\n",
    "    pe = normalize_event_list(row.get(\"predicted_events\", []))\n",
    "\n",
    "    # Event matching\n",
    "    hits, miss, spurious = enhanced_match_row(pe, te)\n",
    "    row_matches.append({\n",
    "        \"name\": row.get(\"name\", i),\n",
    "        \"hits\": hits,\n",
    "        \"miss\": miss,\n",
    "        \"spurious\": spurious\n",
    "    })\n",
    "\n",
    "    # Time accuracy\n",
    "    s, l = time_accuracy_per_row(te, pe)\n",
    "    strict_list.append(s)\n",
    "    lenient_list.append(l)\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_true_events = sum(len(normalize_event_list(row.get(\"true_events\", []))) for _, row in annotated.iterrows())\n",
    "total_pred_events = sum(len(normalize_event_list(row.get(\"predicted_events\", []))) for _, row in annotated.iterrows())\n",
    "total_hits = sum(len(match[\"hits\"]) for match in row_matches)\n",
    "total_miss = sum(len(match[\"miss\"]) for match in row_matches)\n",
    "total_spurious = sum(len(match[\"spurious\"]) for match in row_matches)\n",
    "\n",
    "print(f\"Basic Statistics:\")\n",
    "print(f\" True events total: {total_true_events}\")\n",
    "print(f\" Predicted events total: {total_pred_events}\")\n",
    "print(f\" Prediction/truth ratio: {total_pred_events / total_true_events:.2f}\")\n",
    "\n",
    "# Key enhancement: accuracy metrics based on true event count\n",
    "adjusted_accuracy = total_hits / total_true_events if total_true_events > 0 else 0\n",
    "traditional_precision = total_hits / total_pred_events if total_pred_events > 0 else 0\n",
    "traditional_recall = total_hits / total_true_events if total_true_events > 0 else 0\n",
    "traditional_f1 = (2 * traditional_precision * traditional_recall /\n",
    "                  (traditional_precision + traditional_recall)\n",
    "                  if (traditional_precision + traditional_recall) > 0 else 0)\n",
    "\n",
    "print(f\"\\nKey Performance Indicators:\")\n",
    "print(f\" Traditional precision: {traditional_precision:.3f}\")\n",
    "print(f\" Traditional recall: {traditional_recall:.3f}\")\n",
    "print(f\" Traditional F1 score: {traditional_f1:.3f}\")\n",
    "print(f\" Adjusted accuracy: {adjusted_accuracy:.3f}\")\n",
    "\n",
    "print(f\"\\nTime Accuracy:\")\n",
    "avg_strict_time = sum(strict_list) / len(strict_list) if strict_list else 0\n",
    "avg_lenient_time = sum(lenient_list) / len(lenient_list) if lenient_list else 0\n",
    "print(f\" Strict time matching (exact year): {avg_strict_time:.3f}\")\n",
    "print(f\" Lenient time matching (±2 years/same decade): {avg_lenient_time:.3f}\")\n",
    "\n",
    "# Coverage analysis\n",
    "all_true_times = []\n",
    "all_pred_times = []\n",
    "\n",
    "for i, row in annotated.iterrows():\n",
    "    te = normalize_event_list(row.get(\"true_events\", \"[]\"))\n",
    "    pe = normalize_event_list(row.get(\"predicted_events\", \"[]\"))\n",
    "    all_true_times.extend([_norm_year(e.get(\"time\", \"\")) for e in te if isinstance(e, dict)])\n",
    "    all_pred_times.extend([_norm_year(e.get(\"time\", \"\")) for e in pe if isinstance(e, dict)])\n",
    "\n",
    "true_with_year = sum(1 for t in all_true_times if t is not None)\n",
    "pred_with_year = sum(1 for t in all_pred_times if t is not None)\n",
    "\n",
    "print(f\"\\nTime Extraction Coverage:\")\n",
    "print(f\" True events with years: {true_with_year}/{len(all_true_times)} \"\n",
    "      f\"({true_with_year / len(all_true_times) * 100:.1f}%)\")\n",
    "print(f\" Predicted events with years: {pred_with_year}/{len(all_pred_times)} \"\n",
    "      f\"({pred_with_year / len(all_pred_times) * 100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "067b2986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: /workspace/timeline_events.csv\n",
      "Rows: 210950 | Columns: ['label_name', 'source', 'event_type', 'trigger', 'new_name', 'raw_time', 'start_year', 'start_month', 'end_year', 'end_month', 'granularity', 'confidence', 'sentence', 'profile']\n"
     ]
    }
   ],
   "source": [
    "# Single-output exporter for Discogs label timelines\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Config\n",
    "INPUT_CSV  = \"/workspace/discogs_labels_cleaned.csv\"  \n",
    "OUTPUT_CSV  = \"/workspace/timeline_events.csv\"\n",
    "NAME_COL    = \"name\"\n",
    "\n",
    "# Helpers\n",
    "def _run_pipeline(text: str):\n",
    "    \"\"\"Run spaCy once and collect timeline + raw events for optional auditing.\"\"\"\n",
    "    doc = nlp(text if isinstance(text, str) else \"\")\n",
    "    return {\n",
    "        \"timeline\": getattr(doc._, \"timeline\", []),\n",
    "        \"predicted_events\": getattr(doc._, \"events\", []),       \n",
    "        \"regex_relations\": getattr(doc._, \"regex_relations\", []),  \n",
    "    }\n",
    "\n",
    "def _safe_json_loads(x):\n",
    "    try:\n",
    "        return json.loads(x) if isinstance(x, str) else x\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}. Current columns: {list(df.columns)}\")\n",
    "\n",
    "# Load data\n",
    "if os.path.exists(INPUT_CSV):\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_CSV}\")\n",
    "\n",
    "_ensure_columns(df, [NAME_COL, TEXT_COL])\n",
    "\n",
    "# Run NLP once per row\n",
    "pipe_out = df[TEXT_COL].apply(_run_pipeline).apply(pd.Series)\n",
    "df = pd.concat([df[[NAME_COL, TEXT_COL]], pipe_out], axis=1)\n",
    "\n",
    "# Expand to long table\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    label = row[NAME_COL]\n",
    "    profile_text = row[TEXT_COL]\n",
    "\n",
    "    timeline_list = row.get(\"timeline\", [])\n",
    "    if isinstance(timeline_list, str):\n",
    "        timeline_list = _safe_json_loads(timeline_list)\n",
    "\n",
    "    if timeline_list and isinstance(timeline_list, list):\n",
    "        # Preferred: normalized timeline with confidence\n",
    "        for ev in timeline_list:\n",
    "            rows.append({\n",
    "                \"label_name\":  label,\n",
    "                \"source\":      \"timeline\",\n",
    "                \"event_type\":  ev.get(\"event_type\", \"\"),\n",
    "                \"trigger\":     ev.get(\"trigger\", \"\"),\n",
    "                \"new_name\":    ev.get(\"new_name\", \"\"),\n",
    "                \"raw_time\":    ev.get(\"raw_time\", \"\"),\n",
    "                \"start_year\":  ev.get(\"start_year\", None),\n",
    "                \"start_month\": ev.get(\"start_month\", None),\n",
    "                \"end_year\":    ev.get(\"end_year\", None),\n",
    "                \"end_month\":   ev.get(\"end_month\", None),\n",
    "                \"granularity\": ev.get(\"granularity\", \"\"),\n",
    "                \"confidence\":  ev.get(\"confidence\", None),\n",
    "                \"sentence\":    ev.get(\"sentence\", \"\"),\n",
    "                \"profile\":     profile_text,\n",
    "            })\n",
    "    else:\n",
    "        # Fallback: raw predicted_events (no normalization), so you still get something\n",
    "        pred_list = row.get(\"predicted_events\", [])\n",
    "        if isinstance(pred_list, str):\n",
    "            pred_list = _safe_json_loads(pred_list)\n",
    "        if pred_list and isinstance(pred_list, list):\n",
    "            for ev in pred_list:\n",
    "                rows.append({\n",
    "                    \"label_name\":  label,\n",
    "                    \"source\":      \"predicted_events\",\n",
    "                    \"event_type\":  ev.get(\"event_type\", \"\"),\n",
    "                    \"trigger\":     ev.get(\"trigger\", \"\"),\n",
    "                    \"new_name\":    ev.get(\"new_name\", \"\"),\n",
    "                    \"raw_time\":    ev.get(\"time\", \"\"),\n",
    "                    \"start_year\":  None,\n",
    "                    \"start_month\": None,\n",
    "                    \"end_year\":    None,\n",
    "                    \"end_month\":   None,\n",
    "                    \"granularity\": \"\",\n",
    "                    \"confidence\":  None,\n",
    "                    \"sentence\":    ev.get(\"sentence\", \"\"),\n",
    "                    \"profile\":     profile_text,\n",
    "                })\n",
    "\n",
    "# Save ONE file\n",
    "out_df = pd.DataFrame(rows)\n",
    "out_df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Saved: {OUTPUT_CSV}\")\n",
    "print(f\"Rows: {len(out_df)} | Columns: {list(out_df.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (finalproject)",
   "language": "python",
   "name": "finalproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
